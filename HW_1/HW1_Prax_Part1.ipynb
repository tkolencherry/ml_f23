{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f388228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#Part 2 libraries used\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892fa85",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98081f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/tkolencherry/ml_f23/main/HW_1/garments_worker_productivity.csv\"\n",
    "file = requests.get(url).content\n",
    "prod_data = pd.read_csv(io.StringIO(file.decode('utf-8')))\n",
    "prod_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4ebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.info()\n",
    "#both info and describe because some of the data is categorial\n",
    "#there are missing entries from wip (the number of unfinished iterms for products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28168e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.loc[prod_data[\"wip\"] == 0]\n",
    "\n",
    "#since there are no rows where the works in progress are zero, it seems acceptable to\n",
    "#fill in zeroes for the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28879361",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data = prod_data.fillna(0)\n",
    "prod_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd71104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I want to check the quarter column\n",
    "prod_data.quarter.value_counts()\n",
    "#how are there five quarters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe8303",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.loc[prod_data.quarter == \"Quarter5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.loc[prod_data.quarter == \"Quarter1\"].date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1c2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all of the Q5 entries to Q1 when we map\n",
    "q_dict = {\"Quarter1\":1, \"Quarter2\":2, \"Quarter3\":3, \"Quarter4\":4, \"Quarter5\":1}\n",
    "\n",
    "prod_data = prod_data.replace({'quarter':q_dict})\n",
    "prod_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.department.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.loc[prod_data.department == \"finishing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f18367",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.loc[prod_data.department == \"finishing \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sewing is 1 and Finishing is 2\n",
    "dep_dict = {\"sweing\":1, \"finishing \": 2, \"finishing\":2}\n",
    "prod_data = prod_data.replace({'department':dep_dict})\n",
    "prod_data.department.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fe92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sewing is 1 and Finishing is 2\n",
    "day_dict = {\"Monday\":1, \"Tuesday\": 2, \"Wednesday\":3, \"Thursday\": 4, \"Saturday\": 5, \"Sunday\": 6,}\n",
    "prod_data = prod_data.replace({'day':day_dict})\n",
    "prod_data.day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6671fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.corr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075af90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod_data.corr().actual_productivity.sort_values(ascending = False)\n",
    "\n",
    "#let's pick our indicators - targeted_productivity, no_of_style_change, idle_men, quarter, and team\n",
    "#our response variable is actual_productivity\n",
    "#didn't select smv because it has such high correlation with factors already selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28202aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prod_data[[\"quarter\", \"targeted_productivity\", \"no_of_style_change\", \"idle_men\", \"team\", \"actual_productivity\"]]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_quarter = df.quarter\n",
    "x_targeted_productivity = df.targeted_productivity\n",
    "x_no_of_style_change = df.no_of_style_change\n",
    "x_idle_men = df.idle_men\n",
    "x_team = df.team\n",
    "\n",
    "y = df.actual_productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d651a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( x_quarter, y, s =5, label = \"quarter\")\n",
    "plt.scatter( x_targeted_productivity,y, s =5, label = \"targeted productivity\")\n",
    "plt.scatter(x_no_of_style_change, y,  s =5, label = \"# of style changes\")\n",
    "plt.scatter(x_idle_men, y, s =5, label = \"idle men\")\n",
    "plt.scatter(x_team, y,  s =5, label = \"team size\")\n",
    "plt.legend(fontsize = 15)\n",
    "plt.xlabel('Predictors', fontsize =15)\n",
    "plt.ylabel(\"Actual Productivity Score\", fontsize = 15)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#if there is time, would be good to show a scatter plot but with the conditional means \n",
    "# E[Y|Quarter = 1] vs E[Y|Quarter = 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d623bb6",
   "metadata": {},
   "source": [
    "### Python Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4840d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSD_5:\n",
    "    p1 = np.arange(0.000001,0.00001999,.0000001)\n",
    "    \n",
    "    def __init__(self, df, response_variable, n_pred,mode = \"large\"):\n",
    "        self.df = df\n",
    "        self.response = response_variable\n",
    "        self.mode = mode\n",
    "        self.npred = n_pred\n",
    "        self.weights = []\n",
    "        \n",
    "    def xy_split (self):\n",
    "        df_x = self.df.drop(self.response, axis = 1)\n",
    "        df_y = self.df[self.response]\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, random_state = 42, test_size = 0.2)\n",
    "    \n",
    "        x_test = np.c_[np.ones(x_test.count()[0]), x_test]\n",
    "        x_train = np.c_[np.ones(x_train.count()[0]), x_train]\n",
    "    \n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    def gradient(self, x,y,w):\n",
    "        if self.npred == 5:\n",
    "            y_hat = np.dot(w,x.T) #y_hat = w0(1) + w1x1 + w2x2 +...\n",
    "            residual = y_hat - y\n",
    "            residual = residual.values\n",
    "            x_0 = x[:, :1]\n",
    "            x_1 = x[:, 1:2]\n",
    "            x_2 = x[:, 2:3]\n",
    "            x_3 = x[:, 3:4]\n",
    "            x_4 = x[:, 4:5]\n",
    "            x_5 = x[:, 5:6]\n",
    "        #dot = np.dot(x_0.T,residual) - used for debugging\n",
    "            return residual.mean(), np.dot(x_0.T,residual).mean(), np.dot(x_1.T,residual).mean(), np.dot(x_2.T,residual).mean(), np.dot(x_3.T,residual).mean(), np.dot(x_4.T,residual).mean(), np.dot(x_5.T,residual).mean()\n",
    "        elif self.npred == 3: \n",
    "            y_hat = np.dot(w,x.T) #y_hat = w0(1) + w1x1 + w2x2 +...\n",
    "            residual = y_hat - y\n",
    "            residual = residual.values\n",
    "            x_0 = x[:, :1]\n",
    "            x_1 = x[:, 1:2]\n",
    "            x_2 = x[:, 2:3]\n",
    "            x_3 = x[:, 3:4]\n",
    "\n",
    "            return residual.mean(), np.dot(x_0.T,residual).mean(), np.dot(x_1.T,residual).mean(), np.dot(x_2.T,residual).mean(), np.dot(x_3.T,residual).mean()\n",
    "        elif self.npred == 1:\n",
    "            y_hat = np.dot(w,x.T) #y_hat = w0(1) + w1x1 + w2x2 +...\n",
    "            residual = y_hat - y\n",
    "            residual = residual.values\n",
    "            x_0 = x[:, :1]\n",
    "            x_1 = x[:, 1:2]\n",
    "            \n",
    "            return residual.mean(), np.dot(x_0.T,residual).mean(), np.dot(x_1.T,residual).mean(),\n",
    "            \n",
    "    def gd(self, gradient, x, y, start, learn_rate=0.1, n_iter = 50, tolerance = .01):\n",
    "        vector = start\n",
    "        diff = 0\n",
    "        for i in range(n_iter):\n",
    "        \n",
    "            diff = learn_rate*np.array(self.gradient(x,y,vector))  \n",
    "\n",
    "            if np.all(np.abs(diff) <= tolerance):\n",
    "                break\n",
    "            else:\n",
    "                if(self.npred == 5):\n",
    "                    vector -= diff[1:7] #the first column is the mean residuals, the rest are the partial derivatives w/ respect to x_i\n",
    "                elif(self.npred == 3):\n",
    "                    vector -= diff[1:5]\n",
    "                elif(self.npred == 1):\n",
    "                    vector -= diff[1:3]\n",
    "        return vector\n",
    "\n",
    "    #function for returning assessment measures\n",
    "    def lr_assessment (self, weights,x_train, y_train, x_observed, y_test): \n",
    "        train_yhat = []\n",
    "        test_yhat = []\n",
    "        n_train = len(y_train)\n",
    "        n_test = len(y_test)\n",
    "    \n",
    "        #use our suggested weights to generate a y-hat array for train values\n",
    "        for subset in x_train:\n",
    "            if self.npred ==5: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] + weights[4]*subset[4] + weights[5]*subset[5]\n",
    "            elif self.npred ==3: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] \n",
    "            elif self.npred ==1: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1]\n",
    "            \n",
    "            train_yhat.append(temp_yhat)\n",
    "    \n",
    "        #use our suggested weights to generate a y-hat array for test values\n",
    "        for subset in x_test:\n",
    "            if self.npred ==5: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] + weights[4]*subset[4] + weights[5]*subset[5]\n",
    "            elif self.npred ==3: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] \n",
    "            elif self.npred ==1: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1]\n",
    "                \n",
    "            test_yhat.append(temp_yhat)\n",
    "            \n",
    "        #then calculate the training MSE\n",
    "        train_SE = ((train_yhat-y_train).values)**2\n",
    "        train_MSE = train_SE.sum()/n_train\n",
    "\n",
    "\n",
    "        #then calculate the training MSE\n",
    "        test_SE = ((test_yhat-y_test).values)**2\n",
    "        test_MSE = test_SE.sum()/n_test\n",
    "    \n",
    "        return train_MSE, test_MSE, train_yhat, test_yhat\n",
    "    \n",
    "    def loop_learning(self, x_train, x_test, y_train, y_test,  seed, upper, learn_array, n_iter, threshold = 0.1):\n",
    "    #generate random weights (seed = 175)\n",
    "        self.min_test = 1000000\n",
    "        self.min_p = 1000000\n",
    "        test_MSEs = []\n",
    "        train_MSEs = []\n",
    "        train_r2_adjs = []\n",
    "        test_r2_adjs = []\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        if self.mode == \"large\":\n",
    "            temp_weights = np.random.randint(0,upper,(self.npred+1))*1.0 # we need w0, w1, w2, w3, w4, w5, w6\n",
    "            beg_wt = temp_weights.copy()\n",
    "        else: \n",
    "            temp_weights = np.random.rand(self.npred +1)\n",
    "            temp_weights = temp_weights.astype(float)\n",
    "            beg_wt = temp_weights.copy()\n",
    "\n",
    "#n = 100\n",
    "        for i in learn_array:\n",
    "            sugg_weights = self.gd(self.gradient, x_train, y_train, temp_weights, i, n_iter, threshold)\n",
    "            train_MSE, test_MSE, train_yhat, test_yhat = self.lr_assessment(sugg_weights,x_train, y_train, x_test, y_test) \n",
    "            train_MSEs.append(train_MSE)\n",
    "            test_MSEs.append(test_MSE)\n",
    "            \n",
    "            if test_MSE < self.min_test: \n",
    "                self.min_test = test_MSE\n",
    "                self.min_p = i\n",
    "        print(\"****MODEL PARAMETERS**** \\n\", )\n",
    "        print(\"Weights\")\n",
    "        for l in range(self.npred+1): \n",
    "            print(\"w_\",l,\" : \", beg_wt[l], \"------->\", sugg_weights[l])\n",
    "            \n",
    "        print(\"Iterations : \", n_iter, \"\\n Threshold : \", threshold, \"\\n Number of Predictors : \", self.npred)\n",
    "        print(\"\\n Minimum Test MSE\",self.min_test, \" for Learning Rate = \",self.min_p) \n",
    "        plt.plot(self.p1,train_MSEs, label = \"Training MSE\")\n",
    "        plt.plot(self.p1,test_MSEs, label = \"Test MSE\")\n",
    "        plt.title(\"Training vs Test Errors for Varying Learning Rates\")\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.show()\n",
    "        \n",
    "        self.min_test = 1000000\n",
    "        self.min_p = 1000000\n",
    "        return\n",
    "        \n",
    "    def regression(self, x_train, x_test, y_train, y_test, learn_array, max_iter, threshold):\n",
    "        train_mses = []\n",
    "        test_mses = []\n",
    "        min_test = 1000\n",
    "        min_p = 1000\n",
    "\n",
    "        for i in learn_array:\n",
    "            model = SGDRegressor(alpha=i, eta0=0.001, max_iter = max_iter, tol = threshold)\n",
    "            model.fit(x_train, y_train)\n",
    "            y_train_predict = model.predict(x_train)\n",
    "            train_mse = mean_squared_error(y_train, y_train_predict)\n",
    "            y_test_predict = model.predict(x_test)\n",
    "            test_mse = mean_squared_error(y_test, y_test_predict)\n",
    "            train_mses.append(train_mse)\n",
    "            test_mses.append(test_mse)\n",
    "            if test_mse < min_test: \n",
    "                min_test = test_mse\n",
    "                min_p = i\n",
    "\n",
    "        print(\"WEIGHT ESTIMATION\")\n",
    "        for l in range(self.npred+1): \n",
    "            print(\"w_\",l,\" : \", model.coef_[l])\n",
    "   \n",
    "        print(\"\\n Minimum Test MSE\",min_test, \" for Learning Rate = \", min_p) \n",
    "        plt.plot(self.p1,train_mses, label = \"Training MSE\")\n",
    "        plt.plot(self.p1,test_mses, label = \"Test MSE\")\n",
    "        plt.title(\"Training vs Test Errors for Varying Learning Rates\")\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.show()\n",
    "        \n",
    "    def residual_plot(self, weights, x_train, x_test, y_train, y_test): \n",
    "        train_yhat = []\n",
    "        test_yhat = []\n",
    "        #use our suggested weights to generate a y-hat array for train values\n",
    "        for subset in x_train:\n",
    "            if self.npred ==5: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] + weights[4]*subset[4] + weights[5]*subset[5]\n",
    "            elif self.npred ==3: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] \n",
    "            elif self.npred ==1: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1]\n",
    "            \n",
    "            train_yhat.append(temp_yhat)\n",
    "    \n",
    "        #use our suggested weights to generate a y-hat array for test values\n",
    "        for subset in x_test:\n",
    "            if self.npred ==5: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] + weights[4]*subset[4] + weights[5]*subset[5]\n",
    "            elif self.npred ==3: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1] + weights[2]*subset[2] + weights[3]*subset[3] \n",
    "            elif self.npred ==1: \n",
    "                temp_yhat = weights[0] + weights[1]*subset[1]\n",
    "                \n",
    "            test_yhat.append(temp_yhat)\n",
    "        \n",
    "        train_resids = (y_train - train_yhat)\n",
    "        test_resids = (y_test - test_yhat)\n",
    "            \n",
    "        plt.scatter(train_yhat, train_resids) \n",
    "        plt.title(\"Residual Plot for Model Training\")\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.xlabel(\"Predicted Y\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.scatter(test_yhat, test_resids) \n",
    "        plt.title(\"Residual Plot for Model Testing\")\n",
    "        plt.axhline(y=0, color='r', linestyle='-')\n",
    "        plt.xlabel(\"Predicted Y\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = GSD_5(df,\"actual_productivity\",5)\n",
    "x_train, x_test, y_train, y_test = temp1.xy_split()\n",
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 42, 8, temp1.p1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 42, 20, temp1.p1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef14bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 175, 8, temp1.p1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740bbb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 13, 20, temp1.p1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac24d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789dd9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3c28d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2dd21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 10000,.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fbba7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp1.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100000, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b8657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp2 = GSD_5(df,\"actual_productivity\", 5, \"party time\")\n",
    "temp2.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100000, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a15098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp2.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27141f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp2.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 100, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f0f61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp2.loop_learning(x_train, x_test, y_train, y_test, 99, 20, temp1.p1, 10000,.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3929b4",
   "metadata": {},
   "source": [
    "### Taking out Weak Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a2000a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr().drop(\"actual_productivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c1179",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#there is a decent positive correlation between style changes and the quarter and targeted productivity. Since targeted productivity has a larger correlation, I'll drop quarter and style changes\n",
    "adj3_df = df.drop(columns = [\"quarter\",\"no_of_style_change\"])\n",
    "adj3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff2f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp3 = GSD_5(adj3_df,\"actual_productivity\",3, \"woohoo\")\n",
    "x_train, x_test, y_train, y_test = temp3.xy_split()\n",
    "temp3.loop_learning(x_train, x_test, y_train, y_test, 42, 8, temp3.p1, 100000, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2465d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp3.residual_plot([0.19966337938691053, 0.8098532191256858, -0.009710500708695136, -0.00841640162667597], x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd41fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj1_df = df.drop(columns = [\"quarter\",\"no_of_style_change\",\"idle_men\", \"team\"])\n",
    "adj1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdafb4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp4 = GSD_5(adj1_df,\"actual_productivity\",1, \"woohoo\")\n",
    "x_train, x_test, y_train, y_test = temp4.xy_split()\n",
    "temp4.loop_learning(x_train, x_test, y_train, y_test, 42, 8, temp4.p1, 100000, .00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e12b5ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp4.residual_plot([0.15751497340626586, 0.7902068560712477], x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aea27e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp2.residual_plot([0.36874433840601695,-0.010558652720481124, 0.6219027836157457, -0.039362528753501866, -0.009114088967741867, -0.008742295191248606],x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba1835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.regression(x_train, x_test, y_train, y_test, temp1.p1, 100, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.regression(x_train, x_test, y_train, y_test, temp1.p1, 100000, .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1bb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9903a619",
   "metadata": {},
   "source": [
    "### Linear Regression using ML Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7472d6",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e482420",
   "metadata": {},
   "source": [
    "### Scaling Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1909542",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674bea3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.idle_men.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cd1da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idle_outliers = df.loc[df.idle_men > 20]\n",
    "idle_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14472e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_df = df.drop([650,654,818,822,841,843,882,1046,1085])\n",
    "temp_df.loc[temp_df.idle_men > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064defc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Correlations Before Removing Outliers in idle_men Predictor \\n\",df.corr().actual_productivity)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Correlations After Removing Outliers in idle_men Predictor \\n\",temp_df.corr().actual_productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec6eb7",
   "metadata": {},
   "source": [
    "#### Works Cited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49bc99",
   "metadata": {},
   "source": [
    "Dataset: \n",
    "https://archive.ics.uci.edu/dataset/597/productivity+prediction+of+garment+employees\n",
    "\n",
    "Gradient Descent Lab from Class: https://colab.research.google.com/drive/1rmblKcJUf0A18GMk7Jx4u6DXnQLf32V6?usp=sharing#scrollTo=QVz-JbxFJXOW\n",
    "\n",
    "Gradient Descent in R: (helpful for orienting logic) \n",
    "https://oindrilasen.com/2018/02/compute-gradient-descent-of-a-multivariate-linear-regression-model-in-r/#:~:text=Similar%20to%20the%20Gradient%20Descent%20for%20a%20Univariate,%28i%29%29.%20xj%20%28i%29%20where%20j%20%3D%200%2C1%2C2%E2%80%A6n%20%7D\n",
    "\n",
    "Alternate Multiple Linear Regression with Gradient Descent (used generating random weights and using the dot product of A^T * A for squaring the matrix) \n",
    "https://www.kaggle.com/code/rakend/multiple-linear-regression-with-gradient-descent\n",
    "\n",
    "R-Squared: \n",
    "https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "\n",
    "Reading in Hosted File: \n",
    "https://stackoverflow.com/questions/32400867/pandas-read-csv-from-url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d674d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
