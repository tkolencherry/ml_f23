{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f21541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff41c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (\"C:/Users/teris/ml_f23/HW_2/hotel_booking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87731996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_canceled                       1.000000\n",
       "lead_time                         0.293123\n",
       "arrival_date_year                 0.016660\n",
       "arrival_date_week_number          0.008148\n",
       "arrival_date_day_of_month        -0.006130\n",
       "stays_in_weekend_nights          -0.001791\n",
       "stays_in_week_nights              0.024765\n",
       "adults                            0.060017\n",
       "children                          0.005048\n",
       "babies                           -0.032491\n",
       "is_repeated_guest                -0.084793\n",
       "previous_cancellations            0.110133\n",
       "previous_bookings_not_canceled   -0.057358\n",
       "booking_changes                  -0.144381\n",
       "agent                            -0.083114\n",
       "company                          -0.020642\n",
       "days_in_waiting_list              0.054186\n",
       "adr                               0.047557\n",
       "required_car_parking_spaces      -0.195498\n",
       "total_of_special_requests        -0.234658\n",
       "Name: is_canceled, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['is_canceled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29f7c42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_scratch(object): \n",
    "    \n",
    "    #f(x): Initialize\n",
    "    #Input: Activation Mode - \"sigmoid\", \"relu\", or \"tanh\"\n",
    "    #Purpose: Initialize the class and specify which activation function to use in the NN\n",
    "    def __init__(self, activation_mode, df): \n",
    "        self.n_inputs = 8 #number of predictors \n",
    "        self.n_hidden = 8 #hidden layer with 4 neurons\n",
    "        self.n_outputs = 1 #number of outputs - since this is a classification problem we want 1 class\n",
    "        self.mode = activation_mode\n",
    "        self.df = df\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        #set of weights to go from the input layer to the hidden layer (10Xn matrix)\n",
    "        self.w_inner = np.random.randn(self.n_inputs, self.n_hidden)\n",
    "        \n",
    "        #set of weights to go from hidden layer to output layer (nX1 matrix)\n",
    "        self.w_outer = np.random.randn(self.n_hidden, self.n_outputs)\n",
    "    \n",
    "    #f(x): Pre-Process and Split \n",
    "    def xy_split (self): \n",
    "        # Creating dummy variables from one column:\n",
    "        hotel_type_dict = {\"Resort Hotel\" : 1, \"City Hotel\":0}\n",
    "        df = self.df.replace({'hotel':hotel_type_dict})\n",
    "        df_dummies = pd.get_dummies(df, columns=['deposit_type'])\n",
    "        dataset = df_dummies[['hotel', 'lead_time', 'is_repeated_guest', 'previous_cancellations', 'total_of_special_requests', 'deposit_type_No Deposit', 'deposit_type_Non Refund', 'deposit_type_Refundable', 'is_canceled']]\n",
    "        dataset = dataset.head(3000)\n",
    "        dataset_x = dataset.drop('is_canceled', axis = 1)\n",
    "        dataset_y = dataset['is_canceled']\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(dataset_x, dataset_y, test_size =.20, random_state =42)\n",
    "        \n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    #f(x): Forward Propogation\n",
    "    #Input: x-values \n",
    "    #Purpose: Complete a forward pass through the neural network\n",
    "    #Output: the output value after the pass (the class prediction)\n",
    "    def _fwd_prop(self, X): #input the starting x values and then find what we call netj by taking dot product\n",
    "        self.net_inner = np.dot(self.w_inner.T, X.T)\n",
    "        self.net_outer = np.dot(self.w_outer.T, X.T)\n",
    "        \n",
    "        if(self.mode == \"sigmoid\"):\n",
    "            self.activ_inner = self._sigmoid(self.net_inner)\n",
    "            self.activ_outer = self._sigmoid(self.net_outer)\n",
    "        elif(self.mode == \"relu\"): \n",
    "            self.activ_inner = self._relu(self.net_inner)\n",
    "            self.activ_outer = self._sigmoid(self.net_outer) \n",
    "        elif(self.mode == \"tanh\"):\n",
    "            self.activ_inner = self._tanh(self.net_inner)\n",
    "            self.activ_outer = self._sigmoid(self.net_outer)\n",
    "        return self.activ_outer \n",
    "    \n",
    "    #ACTIVATION FUNCTIONS\n",
    "    #Input: w.T *X - the weighted predictors\n",
    "    #Purpose: non-linear activation functions help us solve more complex classification problems\n",
    "    #Output: neuron output to next layer\n",
    "    def _sigmoid(self, net):\n",
    "        return 1.0/(1+np.exp(-net)) \n",
    "    \n",
    "    #have to make this leaky otherwise this isn't very effective\n",
    "    def _relu (self, net): \n",
    "        return np.maximum(0.01, np.array(net))\n",
    "    \n",
    "    def _tanh(self, net):\n",
    "        return (np.nan_to_num((np.exp(net) - np.exp(-net)))/np.nan_to_num((np.exp(net) + np.exp(-net))))\n",
    "    \n",
    "    \n",
    "    #f(x): Loss Function\n",
    "    #Input: Predicted Y Values and Observed Y Values\n",
    "    #Purpose: We need an optimization function - earlier we used MSE, but here we should use the log error since MSE isn't an appropriate error calculation for binary classification\n",
    "    \n",
    "    def _loss(self, predict, y): #we need to have optimization in this assignment and so we need something to optimize\n",
    "        n = len(y) #grab the number of observations\n",
    "        log_prob = np.nan_to_num(np.multiply(np.log(predict), y)) + np.nan_to_num(np.multiply((1-y), np.log(1-predict)))\n",
    "        loss = - np.sum(log_prob) / n\n",
    "        return loss\n",
    "    #f(x): Back Propogation Function \n",
    "    #Input: X and Y values \n",
    "    #Purpose: We make a backwards pass from the output all the way back to the beginning and update the weights as we go \n",
    "    def _back_prop(self, X, y):\n",
    "        predict = self._fwd_prop(X)\n",
    "        n = X.shape[0]\n",
    "        resid = predict - y\n",
    "        if(self.mode == \"sigmoid\"):\n",
    "            delta_outer = np.multiply(resid, self._sigmoid_prime(self.net_outer))\n",
    "            delta_inner = delta_outer*self.w_outer*self._sigmoid_prime(self.net_inner)\n",
    "        elif(self.mode == \"relu\"):\n",
    "            delta_outer = np.multiply(resid, self._sigmoid_prime(self.net_outer))\n",
    "            delta_inner = delta_outer*self.w_outer*self._relu_prime(self.net_inner)\n",
    "        elif(self.mode == \"tanh\"):\n",
    "            delta_outer = np.multiply(resid, self._tanh_prime(self.net_outer))\n",
    "            delta_inner = delta_outer*self.w_outer*self._tanh_prime(self.net_inner)\n",
    "            \n",
    "        self.dw2 = (1/n)*np.sum(np.multiply(self.activ_inner, delta_outer), axis = 1).reshape(self.w_outer.shape)\n",
    "        self.dw1 = (1/n)*np.dot(X.T, delta_inner.T) #calculate the inner back propogation value for dz and then update\n",
    "        \n",
    "    #ACTIVATION DERIVATIVES\n",
    "    def _relu_prime(self, net):\n",
    "        val = (net>0) *1\n",
    "        return val\n",
    "        \n",
    "    def _sigmoid_prime(self, net):\n",
    "        return self._sigmoid(net)*(1-self._sigmoid(net))\n",
    "    \n",
    "    def _tanh_prime(self, net):\n",
    "        return 1 - (self._tanh(net))**2\n",
    "    \n",
    "    #f(x): Update Weights\n",
    "    #Input: Learning Rate\n",
    "    #Purpose: Update the starting weights using the propogation rule\n",
    "    def _update_wt(self, learning_rate = .001): \n",
    "        self.w_inner = self.w_inner - learning_rate*self.dw1 #update using propogation rule\n",
    "        self.w_outer = self.w_outer - learning_rate*self.dw2 #update using propogation rule\n",
    "    \n",
    "    #f(x): Train\n",
    "    #Input: Beginning X values, observed y values, number of passes\n",
    "    #Purpose: Make a bunch of passes to train the neural network\n",
    "    def train(self, X, y, n_epoch = 10): \n",
    "        for i in range(n_epoch):\n",
    "            y_hat = self._fwd_prop(X)\n",
    "            loss = self._loss(y_hat,y)\n",
    "            self._back_prop(X,y)\n",
    "            self._update_wt()\n",
    "            if i%3 == 0: \n",
    "                print(\"loss: \", loss)\n",
    "                \n",
    "     #the decision boundaries are different for each activation function  \n",
    "    \n",
    "    #f(x) Predict\n",
    "    #Input: Starting x values\n",
    "    #Purpose: Once we've trained the model, make some predictions using a decision boundary\n",
    "    def predict(self, X):\n",
    "        y_hat = self._fwd_prop(X)\n",
    "        if(self.mode == \"sigmoid\"):\n",
    "            y_hat = [1 if i >= 0.5 else 0 for i in y_hat.T]\n",
    "        elif(self.mode == \"relu\"): \n",
    "            y_hat = [1 if i>= 0.5 else 0 for i in y_hat.T]\n",
    "        elif(self.mode == \"tanh\"): \n",
    "            y_hat = [1 if i >= 0.5 else 0 for i in y_hat.T]\n",
    "        return np.array(y_hat)\n",
    "    \n",
    "    #f(x): Score\n",
    "    #Input: predictions and observations \n",
    "    #Purpose: Score the model to see how well it's doing\n",
    "    def score(self, predict, y): \n",
    "        correct = np.sum(predict ==y)\n",
    "        return correct/(len(y))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a5572a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3893736326126938\n",
      "loss:  2.3870930598121807\n",
      "loss:  2.3848117817291716\n",
      "loss:  2.382529799214445\n",
      "predictions.head: [0 0 0 0 1 1 0 0 0]\n",
      "observed.head: [1 0 0 0 0 0 0 1 0]\n",
      "training error: 0.4470833333333334\n",
      "estimated test error: 0.44499999999999995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teris\\AppData\\Local\\Temp\\ipykernel_24280\\4209726451.py:66: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.nan_to_num((np.exp(net) - np.exp(-net)))/np.nan_to_num((np.exp(net) + np.exp(-net))))\n"
     ]
    }
   ],
   "source": [
    "nn = NN_scratch(\"tanh\",df)\n",
    "x_train, x_test, y_train, y_test = nn.xy_split()\n",
    "nn.train(x_train, y_train.values)\n",
    "pred_y_train = nn.predict(x_train)\n",
    "pred_y_test = nn.predict(x_test)\n",
    "\n",
    "accuracy_train = nn.score(pred_y_train, y_train.values)\n",
    "accuracy_test = nn.score(pred_y_test, y_test.values)\n",
    "print('predictions.head:',  pred_y_test[1:10])\n",
    "print('observed.head:', y_test.values[1:10])\n",
    "print('training error:', 1- accuracy_train)\n",
    "print('estimated test error:', 1- accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02595995",
   "metadata": {},
   "source": [
    "### Works Cited \n",
    "Fixing the Relu Derivative Array Issue \n",
    "https://stackoverflow.com/questions/46411180/implement-relu-derivative-in-python-numpy\n",
    "\n",
    "Fixing the Max vs Maximum Issue for Relu\n",
    "https://stackoverflow.com/questions/44957704/passing-relu-function-to-all-element-of-a-numpy-array\n",
    "\n",
    "Nitty Gritty Logic for the Neural Network - First Attempt and Background \n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "Simplified Way to Code Neural Network \n",
    "https://medium.com/@qempsil0914/implement-neural-network-without-using-deep-learning-libraries-step-by-step-tutorial-python3-e2aa4e5766d1\n",
    "\n",
    "What Loss Function for Binary Classification\n",
    "https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/#:~:text=In%20this%20article%2C%20we%20will,used%20for%20binary%20classification%20problems\n",
    "\n",
    "Decision Boundaries for Different Classifiers\n",
    "https://medium.com/analytics-vidhya/activation-functions-in-neural-network-55d1afb5397a\n",
    "\n",
    "Tanh\n",
    "https://datascience.stackexchange.com/questions/109547/why-does-using-tanh-worsen-accuracy-so-much\n",
    "\n",
    "Small Learning Rate for Tanh and Relu\n",
    "https://stats.stackexchange.com/questions/324896/training-loss-increases-with-time\n",
    "\n",
    "Kaggle Dataset\n",
    "https://www.kaggle.com/datasets/mojtaba142/hotel-booking/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
